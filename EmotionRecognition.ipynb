{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGMZxCkXXdKn"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/w3zlhing4dkgeyb/train.zip?dl=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V-kwycpXtDJ"
      },
      "outputs": [],
      "source": [
        "!unzip train.zip?dl=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dcwbXc8Xzww"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input \n",
        "from keras.losses import categorical_crossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQRxQ8qNbvE0"
      },
      "source": [
        "#  Building our Model To train the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HCUQigdYELf"
      },
      "outputs": [],
      "source": [
        "# Working with pre trained model \n",
        "\n",
        "base_model = MobileNet( input_shape=(224,224,3), include_top= False )\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(units=7 , activation='softmax' )(x)\n",
        "\n",
        "# creating our model.\n",
        "model = Model(base_model.input, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN3kEpAeZUGj"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss= categorical_crossentropy , metrics=['accuracy']  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kh9_hjbs47"
      },
      "source": [
        "# Preparing our data using data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpxBuUAlbmh8"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "     zoom_range = 0.2, \n",
        "     shear_range = 0.2, \n",
        "     horizontal_flip=True, \n",
        "     rescale = 1./255\n",
        ")\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(directory= \"/content/train\", \n",
        "                                               target_size=(224,224), \n",
        "                                               batch_size=32,\n",
        "                                  )\n",
        "\n",
        "\n",
        "train_data.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x-TwUhocaHn"
      },
      "outputs": [],
      "source": [
        "val_datagen = ImageDataGenerator(rescale = 1./255 )\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(directory= \"/content/train\", \n",
        "                                           target_size=(224,224), \n",
        "                                           batch_size=32,\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNDgYni5c8Qk"
      },
      "source": [
        "# visualizaing the data that is fed to train data gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tVIpYKDc2a_"
      },
      "outputs": [],
      "source": [
        "# to visualize the images in the traing data denerator \n",
        "\n",
        "t_img , label = train_data.next()\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "# function when called will prot the images \n",
        "def plotImages(img_arr, label):\n",
        "  \"\"\"\n",
        "  input  :- images array \n",
        "  output :- plots the images \n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  for im, l in zip(img_arr,label) :\n",
        "    plt.imshow(im)\n",
        "    plt.title(im.shape)\n",
        "    plt.axis = False\n",
        "    plt.show()\n",
        "    \n",
        "    count += 1\n",
        "    if count == 10:\n",
        "      break\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "# function call to plot the images \n",
        "plotImages(t_img, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhuAjX8KfPa-"
      },
      "source": [
        "# having early stopping and model check point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XtYmjt5dZ2c"
      },
      "outputs": [],
      "source": [
        "## having early stopping and model check point \n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# early stopping\n",
        "es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')\n",
        "\n",
        "# model check point\n",
        "mc = ModelCheckpoint(filepath=\"best_model.h5\", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')\n",
        "\n",
        "# puting call back in a list \n",
        "call_back = [es, mc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GxQ9-qSfT3K"
      },
      "outputs": [],
      "source": [
        "hist = model.fit_generator(train_data, \n",
        "                           steps_per_epoch= 10, \n",
        "                           epochs= 30, \n",
        "                           validation_data= val_data, \n",
        "                           validation_steps= 8, \n",
        "                           callbacks=[es,mc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6YSss56fWrD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Loading the best fit model \n",
        "from keras.models import load_model\n",
        "model = load_model(\"/content/best_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd0p7xDzfe2V"
      },
      "outputs": [],
      "source": [
        "h =  hist.history\n",
        "h.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qm3Qz11fhMy"
      },
      "outputs": [],
      "source": [
        "plt.plot(h['accuracy'])\n",
        "plt.plot(h['val_accuracy'] , c = \"red\")\n",
        "plt.title(\"acc vs v-acc\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2Z9fjdZfjcU"
      },
      "outputs": [],
      "source": [
        "plt.plot(h['loss'])\n",
        "plt.plot(h['val_loss'] , c = \"red\")\n",
        "plt.title(\"loss vs v-loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mj02ddxjTyr"
      },
      "outputs": [],
      "source": [
        "# just to map o/p values \n",
        "op = dict(zip( train_data.class_indices.values(), train_data.class_indices.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ],
      "metadata": {
        "id": "i5kOVaSBH2K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "TBoXeNKrH6RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "metadata": {
        "id": "yYetro4pH-j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data) \n",
        "  # grayscale img\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  print(gray.shape)\n",
        "  # get face bounding box coordinates using Haar Cascade\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # draw face bounding box on image\n",
        "  for (x,y,w,h) in faces:\n",
        "      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "  # save image\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ],
      "metadata": {
        "id": "pttySJIHGwB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  filename = take_photo('photo.jpg')\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "32zWSWEtGzAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xRgvdZZfltN"
      },
      "outputs": [],
      "source": [
        "# path for the image to see if it predics correct class\n",
        "\n",
        "path = \"/content/photo.jpg\"\n",
        "img = load_img(path, target_size=(224,224) )\n",
        "\n",
        "i = img_to_array(img)/255\n",
        "input_arr = np.array([i])\n",
        "input_arr.shape\n",
        "\n",
        "pred = np.argmax(model.predict(input_arr))\n",
        "\n",
        "print(f\" the image is of {op[pred]}\")\n",
        "\n",
        "# to display the image  \n",
        "plt.imshow(input_arr[0])\n",
        "plt.title(\"input image\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woVkimceN8Jr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "projectEmotionRecognition.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}